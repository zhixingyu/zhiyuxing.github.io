---
permalink: /
title: "Yuxing Zhi"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I'm a third year Doctoral student from School of Computer science and engineering, [Xi'an university of technology](https://www.xaut.edu.cn/). 

My research interest includes multimodal fusion, affective computing, and machine learning.

I am very fortunate to be advised by [Prof. Junhuai Li](https://scholar.google.com/citations?user=TJNhQfgAAAAJ&hl=zh-CN) from School of Computer Science and engineering.


Publications
======
\[1\]**Yuxing Zhi**, Junhuai Li, Huaijun Wang, Jing Chen and Wei Wei, "A Multimodal Sentiment Analysis Approach Based on Multi-view Cross-modal Fusion," IEEE Transactions on Computational Social Systems(JCR Q1, CCF C), 2025, Accepted.

\[2\]**Yuxing Zhi**, Junhuai Li, Huaijun Wang, Jing Chen and Ting Cao, "A Fine-Grained Tri-Modal Interaction Model for Multimodal Sentiment Analysis," ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (CCF B). [Link](https://ieeexplore.ieee.org/document/10447872). 

\[3\]**Yuxing Zhi**, Junhuai Li, Huaijun Wang, Jing Chen and Wei. Wei, "A Multimodal Sentiment Analysis Method Based on Fuzzy Attention Fusion,"  IEEE Transactions on Fuzzy Systems (JCR Q1, CCF B, IF=11.9), 2024. [link](https://ieeexplore.ieee.org/document/10613477).

\[4\]Yue Li, **Yuxing Zhi**, Huai Wang, Yufan Guo, Kan Wang, Rong Fei and Junhuai Li. "Cross-domain human activity recognition based on deviation-graph constrained Non-Negative Matrix Factorization," Engineering Applications of Artificial Intelligence(JCR Q1, IF=8.0), 2025. [link](https://www.sciencedirect.com/science/article/pii/S095219762500661X)

\[5\]Li Junhuai, Cao Jingyi, **Yuxing Zhi**, Huaijun Wang and Fei Rong, "Cross-Domain Activity Recognition Based on Stacked Transfer Network," 2024 International Joint Conference on Neural Networks (IJCNN) (CCF C). [link](https://ieeexplore.ieee.org/document/10651514)

\[6\]Junhuai Li, Chuang Lin, Huaijun Wang, **Yuxing Zhi**, Jing Chen and Tao Huang, "Adversarial Training and Cross-modal Feature Fusion in Multimodal Sentiment Analysis," ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (CCF B), 2025. [link](https://ieeexplore.ieee.org/document/10890023)

Projects
======
1. Register a GitHub account if you don't have one and confirm your e-mail (required!)
1. Fork [this template](https://github.com/academicpages/academicpages.github.io) by clicking the "Use this template" button in the top right. 
1. Go to the repository's settings (rightmost item in the tabs that start with "Code", should be below "Unwatch"). Rename the repository "[your GitHub username].github.io", which will also be your website's URL.
1. Set site-wide configuration and create content & metadata (see below -- also see [this set of diffs](http://archive.is/3TPas) showing what files were changed to set up [an example site](https://getorg-testacct.github.io) for a user with the username "getorg-testacct")
1. Upload any files (like PDFs, .zip files, etc.) to the files/ directory. They will appear at https://[your GitHub username].github.io/files/example.pdf.  
1. Check status by going to the repository settings, in the "GitHub pages" section

